If github in unable to render a Jupyter notebook, copy the link of the notebook and enter into the nbviewer:
https://nbviewer.jupyter.org/

In this notebook series on the K-Nearest Neighbors (K-NN), we introduce the K-NN model and investigate various aspects of performing Machine Learning (ML) using the K-NN model.

- K-NN 0: How to use Scikit-Learn to create a K-NN model
- K-NN 1: Visualize decision boundary
- K-NN 2: Introduce various steps of Machine Learning for performing binary classification using the K-NN model. The steps include:

  -- Creating test dataset
  
  -- Creating the KNN classifier model

  -- Model selection via hyperparameter tuning

  -- Understanding the probabilistic prediction of the KNN model

  -- Evaluate a KNN model using accuracy, precision, recall, F1 score and confusion matrix
  
  -- Creating the Receiver Operating Characteristic (ROC) curve
 
  -- Creating the Precision-Recall curve & computing area under the curve (AUC) score

  -- Determining the optimal threshold

  -- Improve a model's performance by using its optimal threshold
  
- K-NN 3: Introduce various steps of Machine Learning for performing multi-class classification using the K-NN model 

- K-NN 4: Effect of data scaling on K-NN classification

- K-NN 5: Investigate the non-linear decision boundary of K-NN classifier

- K-NN 6: Investigate the curse of dimensionality (multi-class classification of the MNIST handwritten digits dataset)

